{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["1nBcATPMmndd"]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1560215824847,"user_tz":420,"elapsed":582,"user":{"displayName":"Jessica Urbano","photoUrl":"https://lh6.googleusercontent.com/-JY19-MQQZZY/AAAAAAAAAAI/AAAAAAAAAOQ/1wjLsu2nfVE/s64/photo.jpg","userId":"15085472728949360208"}},"id":"iagX34EWmojD","outputId":"ee8acd9c-8461-4b9c-be63-e329b0b394ac","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/'My Drive'/ECE_285_proj/\n","%cd DeepZip_Compression"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/ECE_285_proj\n","/content/drive/My Drive/ECE_285_proj/DeepZip_Compression\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1560215826056,"user_tz":420,"elapsed":1758,"user":{"displayName":"Jessica Urbano","photoUrl":"https://lh6.googleusercontent.com/-JY19-MQQZZY/AAAAAAAAAAI/AAAAAAAAAOQ/1wjLsu2nfVE/s64/photo.jpg","userId":"15085472728949360208"}},"id":"4FCm6wO-ms8z","outputId":"b988ebd6-454a-452a-fb6e-d48dfbf14d51","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["arithmetic_coder.ipynb\tDeepZip_Compression  main-checkpoint.ipynb  __pycache__\n","data_preparation.py\tinput.txt\t     my_model.h5\t    README.md\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1560215827707,"user_tz":420,"elapsed":3407,"user":{"displayName":"Jessica Urbano","photoUrl":"https://lh6.googleusercontent.com/-JY19-MQQZZY/AAAAAAAAAAI/AAAAAAAAAOQ/1wjLsu2nfVE/s64/photo.jpg","userId":"15085472728949360208"}},"id":"x78VgVd0mndH","outputId":"8c51c894-e81c-4bac-c193-e42b982d5cf3","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import sys\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Bidirectional\n","from keras.layers import LSTM, Flatten, Conv1D, LocallyConnected1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n","from math import sqrt\n","from keras.layers.embeddings import Embedding\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","# from matplotlib import pyplot\n","import keras\n","from sklearn.preprocessing import OneHotEncoder\n","from keras.layers.normalization import BatchNormalization\n","import tensorflow as tf\n","import numpy as np\n","import argparse\n","import os\n","from keras.callbacks import CSVLogger\n","import keras.backend as K\n","np.set_printoptions(threshold=sys.maxsize)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pBpyl_VVmndO"},"source":["## DATA PREP"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1560215830684,"user_tz":420,"elapsed":5035,"user":{"displayName":"Jessica Urbano","photoUrl":"https://lh6.googleusercontent.com/-JY19-MQQZZY/AAAAAAAAAAI/AAAAAAAAAOQ/1wjLsu2nfVE/s64/photo.jpg","userId":"15085472728949360208"}},"id":"5y2Q_SMjmndP","outputId":"35f3b697-7e3e-4f54-cb8a-d61a64a7e87e","colab":{"base_uri":"https://localhost:8080/","height":260}},"source":["import data_preparation\n","#Convert letters to integers\n","integer_encoded, params = data_preparation.process_data('input.txt')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["15072434\n","{'T': 0, 'A': 1, 'C': 2, 'G': 3}\n","{0: 'T', 1: 'A', 2: 'C', 3: 'G'}\n","[[3]\n"," [2]\n"," [2]\n"," [0]\n"," [1]\n"," [1]\n"," [3]\n"," [2]\n"," [2]\n"," [0]]\n","GCCTAAGCCT\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FuI8s7sSmndU","colab":{}},"source":["bs=128\n","time_steps=64\n","num_epochs=20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1560215833178,"user_tz":420,"elapsed":401,"user":{"displayName":"Jessica Urbano","photoUrl":"https://lh6.googleusercontent.com/-JY19-MQQZZY/AAAAAAAAAAI/AAAAAAAAAOQ/1wjLsu2nfVE/s64/photo.jpg","userId":"15085472728949360208"}},"id":"hn92lIlOmndX","outputId":"e94d7cb5-dc72-4f8f-fd54-d61303d79ffd","colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["#Generate traning data and its lable \n","X,Y_raw, Y = data_preparation.generate_single_output_data(integer_encoded[: bs * 90  ],bs, time_steps)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(11520, 1)\n","(11520,)\n","11392\n","(11392, 65)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n","If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n","In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1nBcATPMmndd"},"source":["## Trainning"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zQL3Vo0Umnde","colab":{}},"source":["def custom_loss(y_true, y_pred):\n","        return 1/np.log(2) * K.categorical_crossentropy(y_true, y_pred)        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YVWxbczJmndg","colab":{},"outputId":"ad08f724-10e3-4de4-85a3-07798a8addbb"},"source":["alphabet_size = Y.shape[1]\n","DZ_model = Sequential()\n","DZ_model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n","DZ_model.add(LSTM(32, stateful=False, return_sequences=True))\n","DZ_model.add(LSTM(32, stateful=False, return_sequences=True))\n","DZ_model.add(Flatten())\n","DZ_model.add(Dense(64, activation='relu'))\n","DZ_model.add(Dense(alphabet_size, activation='softmax'))\n","optim = keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0, amsgrad=False)\n","DZ_model.compile(loss=custom_loss, optimizer=optim)\n","early_stopping = EarlyStopping(monitor='loss', mode='min', min_delta=0.005, patience=3, verbose=1)\n","callbacks_list = [early_stopping]\n","DZ_model.fit(X, Y, epochs=num_epochs, batch_size=bs, verbose=1, shuffle=True, callbacks=callbacks_list)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\quoca\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From C:\\Users\\quoca\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch 1/20\n","11392/11392 [==============================] - 7s 657us/step - loss: 1.9346\n","Epoch 2/20\n","11392/11392 [==============================] - 6s 534us/step - loss: 1.9195\n","Epoch 3/20\n","11392/11392 [==============================] - 6s 503us/step - loss: 1.8904\n","Epoch 4/20\n","11392/11392 [==============================] - 6s 514us/step - loss: 1.8378\n","Epoch 5/20\n","11392/11392 [==============================] - 6s 527us/step - loss: 1.7968\n","Epoch 6/20\n","11392/11392 [==============================] - 6s 552us/step - loss: 1.7637\n","Epoch 7/20\n"," 6528/11392 [================>.............] - ETA: 2s - loss: 1.7246"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-6-850c803a7df2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mDZ_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ht_DJGOGmndl"},"source":["## Compression"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1560215837343,"user_tz":420,"elapsed":842,"user":{"displayName":"Jessica Urbano","photoUrl":"https://lh6.googleusercontent.com/-JY19-MQQZZY/AAAAAAAAAAI/AAAAAAAAAOQ/1wjLsu2nfVE/s64/photo.jpg","userId":"15085472728949360208"}},"id":"yQ0T4Tl-mndm","outputId":"db3cedc5-7dbb-49ff-fa7e-3f8caf39a141","colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["DZ_model = load_model('my_model.h5', compile=False)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nAyEIB48mndp","colab":{}},"source":["class ArithmeticEncoder(object):\n","    def __init__(self, bitlen):\n","        self.bit_prec = bitlen   #bit precision \n","        self.max_range = 1 << self.bit_prec  #max range based on bit precision 2^bit_prec\n","        self.mask = self.max_range - 1  #max range index starting at 0\n","        self.renorm= self.max_range >> 1  #renormalization threshold\n","        self.second_mask = self.max_range >> 1\n","        self.low = 0  #initial low\n","        self.high = self.mask  #initial high\n","        self.s = 0  \n","\n","    def update(self, sym, c):\n","        low = self.low   \n","        high = self.high\n","        range = high - low + 1\n","#         print(sym)\n","#         print(sym.shape)\n","        sym = int(sym)\n","        total = (c[-1])  #cumulative propabilities\n","        symlow = (c[sym])  \n","        symhigh = (c[sym+1] )\n","        \n","\n","        newlow = low + symlow*range // total  #low in arithmetic integer\n","        newhigh = low + symhigh*range // total -1 #high in arithemtic integer\n","        self.low = int(newlow)\n","        self.high = int(newhigh)\n","        range = self.high - self.low \n","#         print(\"sym:\", sym)\n","#         print(\"symlow:\", self.low)\n","#         print(\"symhigh:\", self.high)\n","#         print(\"range:\", range)\n","        \n","        #renormalization\n","        while((self.low ^ self.high) & self.renorm) == 0:\n","            self.low = (self.low << 1) \n","            range = range << 1 | 1\n","            self.high = self.low + range\n","            self.s = self.s + 1          \n","#         print(\"renorm low:\",self.low)\n","#         print(\"renorm range:\",range)\n","#         print(\"renorm high:\", self.high)\n","   \n","    def write(self, c, sym):\n","        self.update(c,sym)\n","        return [self.low, self.s]\n","        \n","    def finish(self):\n","        self.output.write(1)\n","\n","\n","# In[25]:\n","\n","\n","class ArithmeticDecoder(ArithmeticEncoder):\n","    def __init__(self,statesize,bitin,c):\n","        self.input = bitin\n","        self.bitstream = ([int(d) for d in str(self.input)])\n","        self.code = 0\n","        self.max_range = 1 << statesize\n","        self.renorm= self.max_range >> 1\n","        self.mask = self.max_range - 1\n","        self.stream = self.input[0:statesize]\n","        self.low = int(self.stream,2)\n","        self.t = statesize\n","        self.thresh = []\n","        self.total = (c[-1])\n","        self.cum = [c[i]/self.total for i in range(len(c))]\n","        self.thresh = [round((self.cum[i])*self.mask) for i in range(len(self.cum))]\n","        \n","    def decode(self, c):\n","#         print(\"low:\",self.low\n","        c = [c[i]/self.total for i in range(len(c))]\n","#         print(\"cum::\", c)\n","        for i in range(len(self.thresh)-1):\n","            if((self.low < self.thresh[i+1]) & (self.low >= self.thresh[i])):\n","                sym = i\n","#                 print(\"encoded value:\",self.low)\n","#                 print(\"threshold:\", self.thresh)\n","#                 print(\"decoded value:\", sym)\n","                rangenew = int(self.thresh[i+1] - self.thresh[i])\n","#                 print(\"range:\", rangenew)\n","#                 print(\"low:\", self.thresh[i])\n","                while (rangenew < self.renorm):\n","                    rangenew = rangenew << 1 \n","                    self.low = self.low << 1\n","                    self.thresh[i] = int(self.thresh[i]) << 1\n","#                     print(\"renorm val;\",self.low)\n","#                     print(\"renorm range;\",rangenew)\n","                    self.thresh[i+1] = rangenew + self.thresh[i]\n","#                     print(\"renorm low:\" ,self.thresh[i])\n","                self.thresh = [(c[j]*(self.thresh[i+1]-self.thresh[i]))+self.thresh[i] for j in range(len(self.cum))]\n","                break\n","        return sym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"It5W7JMRmnds","colab":{}},"source":["alphabet_size = 4\n","def cumul_d(prob):\n","    return np.cumsum(prob, axis = 1)  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1560217712655,"user_tz":420,"elapsed":1105,"user":{"displayName":"Jessica Urbano","photoUrl":"https://lh6.googleusercontent.com/-JY19-MQQZZY/AAAAAAAAAAI/AAAAAAAAAOQ/1wjLsu2nfVE/s64/photo.jpg","userId":"15085472728949360208"}},"id":"6-uH3StGmndv","outputId":"580f3c80-9c76-428b-900e-3300067f16f3","colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["bitprecision = 64\n","l = int(X.shape[0] / bs) * bs \n","print(l)\n","print((X.shape[0])-time_steps)\n","\n","#Create Uniform distribution to feed in to the model\n","alphabet_size = 4\n","prob = np.ones(alphabet_size)/alphabet_size\n","c = np.zeros(alphabet_size+1,  dtype = np.uint64)\n","c[1:] = np.cumsum(prob*10000000 + 1)        \n","bitstream = []\n","enc = ArithmeticEncoder(bitprecision)\n","\n","num_iters = int((len(X)+time_steps)/bs)\n","ind = np.array(range(bs))*num_iters\n","prob = DZ_model.predict(X[ind,:], batch_size=bs)\n","count =0 \n","\n","#Encoding\n","for i in (range(bs)):\n","    for j in range(min(time_steps, num_iters)):\n","        new = enc.write(X[ind[i], j], c.tolist())\n","         \n","\n","low_final = new[0] #final low value \n","s_final = new[1]  #count of renormalizations\n","bitstream_init = format(int(low_final), 'b') \n","while len(bitstream_init) < (s_final+bitprecision):\n","    bitstream_init = '0' + bitstream_init\n","print(len(bitstream_init))\n","\n","#convert low to bitstream, ensure proper number of bits \n","cumul = np.zeros((bs, alphabet_size + 1), dtype = np.uint64)\n","cumul_enc = np.zeros((bs*(num_iters-time_steps), alphabet_size + 1), dtype = np.uint64)\n","for k in (range(num_iters - time_steps)):\n","    prob = DZ_model.predict(X[ind,:], batch_size=bs)\n","    cumul[:,1:] = np.cumsum(prob*10000000 + 1, axis = 1)\n","    cumul_enc[k*bs:k*bs+bs,:] = cumul\n","    for i in range(bs):\n","        new = enc.write(Y_raw[ind[i]], cumul[i,:].tolist())\n","    ind = ind + 1\n","    \n","print(cumul.shape)\n","print(cumul_enc.shape)\n","low_final = new[0] #final low value \n","s_final = new[1]  #count of renormalizations\n","ind = ind + 1\n","#convert low to bitstream, ensure proper number of bits \n","bitstream = format(int(low_final), 'b')  \n","while len(bitstream) < (s_final+bitprecision):\n","    bitstream= '0' + bitstream\n","\n","bitstream = bitstream_init + bitstream\n","\n","print(\"encoded bit length:\", len(bitstream))\n","\n","\n","\n","# symdec = []\n","# # print(\"C:\", c)\n","# for symbols in range(len(X)):\n","#     symdec.append(dec.decode(cumul))\n","    \n","    \n","# print(X.shape)\n","# print(\"input stream:\", X[:,0])\n","# print(\"decoded num stream:\", symdec)\n","# print(len(symdec))\n","\n","\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["11392\n","11328\n","16448\n","(128, 5)\n","(3200, 5)\n","encoded bit length: 41204\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1560217751035,"user_tz":420,"elapsed":783,"user":{"displayName":"Jessica Urbano","photoUrl":"https://lh6.googleusercontent.com/-JY19-MQQZZY/AAAAAAAAAAI/AAAAAAAAAOQ/1wjLsu2nfVE/s64/photo.jpg","userId":"15085472728949360208"}},"id":"cFodAVHSl7fA","outputId":"3145b4e7-26da-482a-b983-b1e80781b33a","colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["\n"," #START DECODER\n","series = np.zeros((bs,num_iters), dtype = np.uint8)  \n","alphabet_size = 4\n","prob = np.ones(alphabet_size)/alphabet_size\n","c = np.zeros(alphabet_size+1,  dtype = np.uint64)\n","c[1:] = np.cumsum(prob*10000000 + 1)        \n","num_iters = int((len(X)+time_steps)/bs)\n","ind = np.array(range(bs))*num_iters\n","dec = ArithmeticDecoder(bitprecision, bitstream ,c)\n","\n","symdec = []\n","for i in range(bs):\n","  for j in range(min(time_steps, num_iters)):\n","    symdec.append(dec.decode(c.tolist()))\n","print(X.shape)\n","print(len(symdec))\n","\n","\n","cumul = np.zeros((bs, alphabet_size + 1), dtype = np.uint64)        \n","for k in (range(num_iters - time_steps)):\n","    c = cumul_enc[k*bs:k*bs+bs,:]\n","#   prob = DZ_model.predict(series[:,j:j+time_steps], batch_size=bs)\n","#     cumul[:,1:] = np.cumsum(prob*10000000 + 1, axis = 1)\n","    for i in range(bs):\n","      symdec.append(dec.decode(c[i,:].tolist()))\n","    \n","print(X.shape)\n","print(\"decoded num stream:\", symdec)\n","print(len(symdec))\n","\n","\n","\n"],"execution_count":56,"outputs":[{"output_type":"stream","text":["(11392, 64)\n","8192\n","(11392, 64)\n","decoded num stream: [3, 2, 2, 0, 1, 1, 3, 2, 2, 0, 1, 1, 3, 2, 2, 0, 1, 1, 3, 2, 2, 0, 1, 1, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 3, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 3, 1, 1, 2, 3, 1, 0, 1, 0, 2, 1, 1, 3, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 0, 2, 0, 0, 3, 1, 1, 1, 1, 2, 1, 0, 3, 0, 1, 1, 1, 2, 1, 2, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 2, 1, 0, 3, 1, 1, 0, 1, 2, 1, 0, 3, 1, 0, 2, 1, 2, 1, 1, 0, 3, 3, 1, 1, 2, 2, 1, 1, 2, 1, 0, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 0, 2, 1, 0, 2, 1, 0, 1, 3, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 2, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0, 2, 2, 2, 0, 3, 1, 2, 2, 0, 1, 1, 0, 1, 1, 3, 0, 2, 1, 2, 0, 3, 1, 3, 1, 2, 1, 0, 0, 1, 0, 3, 2, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 3, 2, 0, 0, 1, 2, 1, 2, 1, 0, 1, 3, 1, 2, 2, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 0, 2, 1, 1, 3, 0, 1, 1, 2, 3, 1, 1, 0, 1, 2, 0, 1, 1, 2, 2, 3, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 3, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1, 3, 0, 0, 1, 0, 3, 1, 2, 1, 3, 3, 2, 1, 0, 0, 3, 1, 0, 2, 0, 3, 0, 1, 1, 0, 2, 2, 3, 0, 1, 2, 2, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 3, 0, 2, 0, 3, 1, 2, 0, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 0, 1, 1, 2, 3, 0, 1, 0, 0, 0, 1, 3, 2, 1, 2, 2, 0, 3, 0, 3, 0, 2, 0, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 3, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 2, 3, 1, 1, 2, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1, 2, 2, 3, 1, 0, 1, 2, 2, 3, 0, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 3, 1, 1, 1, 0, 3, 0, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 0, 1, 1, 0, 2, 0, 2, 1, 1, 1, 1, 0, 1, 2, 1, 2, 2, 2, 1, 3, 0, 1, 1, 0, 2, 2, 3, 2, 2, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 2, 1, 2, 0, 3, 2, 1, 1, 1, 0, 0, 1, 3, 2, 2, 3, 1, 2, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 2, 0, 3, 1, 0, 2, 2, 1, 3, 1, 1, 2, 2, 2, 0, 0, 1, 3, 1, 3, 1, 0, 3, 0, 2, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 0, 0, 1, 0, 1, 2, 1, 0, 1, 2, 3, 0, 1, 3, 0, 1, 0, 0, 2, 1, 1, 1, 3, 0, 2, 2, 2, 2, 0, 2, 3, 1, 2, 1, 2, 1, 1, 3, 0, 0, 0, 1, 3, 0, 1, 1, 0, 1, 2, 1, 2, 1, 0, 0, 0, 1, 0, 2, 1, 3, 2, 2, 0, 2, 2, 0, 2, 1, 1, 3, 0, 1, 0, 0, 0, 1, 2, 2, 1, 1, 2, 0, 1, 1, 2, 0, 1, 1, 2, 1, 2, 1, 3, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 3, 0, 0, 0, 1, 1, 1, 1, 1, 3, 1, 0, 1, 0, 2, 3, 2, 1, 0, 1, 0, 2, 1, 1, 0, 1, 1, 2, 2, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 3, 2, 1, 0, 1, 2, 1, 1, 0, 0, 2, 0, 1, 3, 2, 1, 0, 1, 0, 2, 1, 1, 1, 3, 1, 1, 2, 1, 0, 3, 1, 3, 0, 1, 0, 1, 1, 0, 0, 1, 2, 3, 1, 2, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 3, 1, 0, 1, 1, 1, 1, 3, 3, 3, 2, 0, 1, 1, 1, 1, 1, 1, 2, 3, 2, 1, 1, 2, 3, 1, 0, 0, 0, 1, 0, 3, 0, 3, 1, 0, 1, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 2, 0, 0, 2, 1, 1, 1, 0, 2, 1, 1, 0, 2, 1, 3, 1, 2, 3, 0, 2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 3, 1, 1, 1, 2, 1, 0, 2, 1, 1, 1, 0, 0, 3, 1, 1, 0, 1, 1, 0, 1, 3, 0, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 2, 1, 3, 1, 0, 0, 1, 2, 2, 2, 1, 1, 2, 2, 0, 3, 1, 1, 1, 2, 2, 1, 2, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 2, 1, 0, 3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 2, 2, 1, 2, 1, 1, 2, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 2, 1, 3, 0, 3, 1, 1, 2, 2, 3, 1, 1, 0, 1, 1, 3, 0, 2, 2, 3, 1, 3, 2, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 1, 2, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 3, 0, 1, 0, 1, 3, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 0, 1, 2, 0, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 2, 3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 2, 0, 0, 2, 1, 0, 1, 1, 2, 0, 1, 1, 1, 3, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 2, 0, 2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 2, 0, 1, 1, 2, 1, 1, 1, 3, 1, 1, 0, 1, 1, 2, 1, 1, 2, 1, 0, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 1, 0, 2, 3, 1, 3, 1, 1, 1, 1, 0, 2, 2, 1, 2, 1, 0, 1, 0, 2, 2, 2, 3, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1, 1, 0, 1, 1, 2, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 3, 1, 0, 0, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 2, 0, 2, 1, 0, 1, 2, 1, 3, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 2, 0, 2, 0, 0, 3, 2, 0, 0, 2, 2, 1, 1, 0, 0, 1, 3, 0, 1, 1, 2, 2, 3, 1, 2, 1, 0, 1, 3, 0, 0, 0, 0, 1, 1, 0, 1, 3, 1, 2, 0, 1, 2, 2, 1, 2, 0, 2, 1, 0, 0, 1, 1, 1, 1, 1, 3, 3, 3, 1, 2, 1, 1, 0, 1, 2, 0, 1, 2, 1, 0, 1, 0, 0, 1, 3, 0, 1, 1, 0, 2, 1, 1, 1, 3, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 2, 0, 1, 0, 0, 2, 0, 3, 1, 0, 1, 3, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 0, 1, 2, 1, 3, 1, 2, 1, 3, 1, 0, 3, 1, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, 2, 1, 3, 0, 0, 1, 0, 1, 1, 2, 3, 0, 1, 2, 1, 3, 2, 1, 0, 1, 1, 1, 1, 2, 2, 1, 1, 1, 0, 3, 0, 1, 1, 0, 0, 1, 2, 2, 0, 3, 3, 2, 2, 0, 1, 1, 2, 2, 1, 3, 2, 1, 0, 0, 3, 2, 2, 2, 2, 1, 0, 2, 1, 1, 1, 1, 0, 3, 1, 2, 2, 1, 0, 1, 1, 0, 0, 0, 2, 1, 1, 2, 0, 1, 1, 0, 0, 1, 3, 0, 3, 1, 1, 1, 3, 1, 1, 3, 0, 1, 0, 1, 0, 0, 0, 3, 3, 1, 1, 0, 1, 3, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 0, 0, 0, 1, 0, 2, 3, 0, 0, 2, 1, 0, 0, 3, 3, 0, 2, 2, 3, 1, 2, 3, 2, 1, 1, 1, 2, 0, 1, 0, 2, 0, 0, 1, 3, 1, 1, 0, 3, 1, 1, 0, 1, 1, 2, 0, 2, 1, 1, 3, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 1, 2, 2, 0, 0, 0, 2, 1, 1, 3, 0, 2, 2, 1, 1, 1, 0, 1, 0, 2, 1, 3, 1, 0, 2, 1, 1, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2, 1, 2, 2, 2, 1, 2, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 1, 2, 0, 0, 2, 0, 2, 2, 3, 0, 2, 3, 0, 1, 0, 1, 0, 2, 3, 2, 2, 3, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 0, 0, 0, 2, 2, 0, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 3, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 2, 0, 3, 0, 1, 3, 1, 2, 0, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 0, 3, 1, 1, 1, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 3, 1, 0, 1, 2, 0, 0, 2, 1, 0, 1, 3, 2, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2, 1, 0, 1, 2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 3, 1, 0, 2, 1, 1, 0, 1, 3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 0, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1, 1, 2, 1, 1, 1, 2, 1, 2, 3, 0, 0, 2, 1, 2, 0, 2, 0, 0, 0, 2, 1, 1, 0, 0, 2, 1, 3, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 0, 1, 1, 1, 1, 0, 3, 3, 1, 0, 0, 1, 0, 1, 2, 1, 0, 2, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 1, 3, 2, 3, 3, 1, 1, 0, 2, 1, 1, 0, 2, 1, 0, 1, 1, 2, 0, 1, 1, 2, 1, 2, 0, 1, 0, 1, 2, 0, 0, 2, 1, 1, 1, 1, 2, 1, 0, 1, 2, 1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 3, 2, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 0, 1, 3, 0, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 3, 1, 1, 2, 1, 1, 3, 2, 2, 3, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1, 0, 1, 2, 1, 0, 0, 2, 1, 0, 3, 1, 0, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 2, 0, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 0, 3, 0, 0, 0, 3, 1, 0, 1, 0, 1, 1, 1, 0, 2, 1, 1, 2, 2, 1, 1, 0, 0, 2, 1, 1, 3, 0, 1, 2, 0, 1, 1, 0, 0, 1, 1, 1, 2, 0, 3, 1, 2, 0, 3, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 2, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 3, 2, 2, 1, 2, 2, 0, 1, 0, 2, 1, 1, 1, 0, 1, 0, 1, 0, 2, 2, 0, 1, 2, 0, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 2, 2, 3, 1, 1, 1, 2, 2, 1, 0, 1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 3, 1, 1, 2, 1, 0, 1, 2, 2, 0, 1, 2, 3, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 3, 1, 1, 1, 2, 2, 1, 2, 1, 1, 3, 0, 1, 1, 1, 1, 1, 3, 0, 1, 0, 2, 3, 1, 1, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 2, 3, 1, 1, 0, 2, 2, 0, 1, 0, 2, 2, 2, 2, 1, 1, 1, 0, 2, 0, 2, 2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 0, 1, 2, 3, 2, 1, 0, 1, 1, 1, 0, 2, 0, 2, 1, 1, 3, 3, 1, 0, 1, 1, 2, 1, 3, 1, 1, 0, 1, 3, 2, 2, 0, 0, 1, 1, 2, 0, 1, 2, 1, 2, 0, 1, 2, 1, 1, 3, 2, 1, 3, 0, 1, 1, 1, 1, 0, 3, 1, 0, 3, 0, 1, 0, 3, 1, 1, 1, 0, 3, 1, 1, 0, 1, 1, 2, 0, 1, 3, 1, 1, 1, 0, 0, 1, 2, 1, 1, 1, 1, 1, 0, 2, 1, 1, 2, 1, 1, 3, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 0, 0, 1, 3, 0, 1, 2, 0, 1, 1, 0, 0, 1, 0, 1, 3, 2, 3, 2, 3, 1, 0, 1, 0, 0, 2, 1, 0, 3, 2, 0, 0, 1, 3, 0, 0, 1, 0, 1, 2, 1, 1, 0, 2, 3, 1, 3, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 2, 0, 1, 2, 2, 1, 3, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 1, 1, 1, 1, 3, 2, 0, 1, 1, 1, 1, 0, 1, 3, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 1, 3, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 1, 3, 3, 2, 3, 2, 3, 1, 1, 1, 2, 1, 2, 0, 1, 1, 1, 1, 0, 2, 0, 0, 2, 3, 0, 1, 0, 0, 1, 2, 2, 3, 1, 1, 0, 0, 0, 2, 1, 0, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 3, 3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 3, 1, 1, 1, 2, 3, 0, 1, 2, 0, 0, 1, 0, 0, 0, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 0, 1, 0, 2, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 0, 1, 3, 1, 0, 0, 3, 1, 1, 1, 1, 0, 0, 3, 1, 1, 1, 2, 2, 0, 0, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 3, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 2, 2, 1, 0, 2, 1, 1, 1, 1, 1, 3, 1, 2, 0, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 3, 0, 2, 1, 2, 1, 2, 1, 0, 2, 2, 1, 1, 1, 1, 3, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 3, 3, 1, 2, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 2, 1, 0, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1, 2, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 2, 1, 2, 0, 2, 2, 1, 1, 1, 2, 3, 1, 0, 0, 0, 2, 1, 1, 2, 0, 3, 1, 1, 0, 1, 1, 1, 2, 1, 3, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 0, 2, 1, 0, 3, 3, 2, 3, 0, 0, 1, 1, 3, 0, 1, 0, 3, 1, 3, 1, 0, 0, 1, 2, 1, 2, 1, 1, 3, 0, 2, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 3, 1, 2, 3, 0, 2, 0, 0, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 0, 1, 1, 1, 2, 2, 2, 3, 0, 1, 3, 1, 0, 2, 0, 3, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 3, 1, 1, 2, 2, 0, 1]\n","11392\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hIq6jMdBHApn","colab_type":"code","colab":{},"outputId":"2c7ea7f3-2b52-4817-f05f-59944fe33c86"},"source":["print(X[0,:])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1 0 0 2 3 3 1 0 0 2 3 3 1 0 0 2 3 3 1 0 0 2 3 3 1 0 0 2 3 3 1 0 0 2 3 3 1\n"," 0 0 2 3 3 1 0 0 2 3 3 1 0 0 2 3 3 1 0 0 2 3 3 1 0 0 2]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_NxuxY4Rmndz","outputId":"dfef3887-e8db-40c3-f085-72e7e25e9f0b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":[" series_2d = np.zeros((bs,num_iters), dtype = np.uint8)\n"," print(series_2d.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(128, 90)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9op9TcI1mnd3","colab":{}},"source":["cumul[:,0:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vZn98lYNmnd7","colab":{}},"source":["np.asscalar(cumul[-1])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"56XP3XHnmneB","colab":{}},"source":["X[i,:].reshape(1,-1).shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5Dtk3EOMmneE","colab":{}},"source":["if (l > (len(X.shape[0])-timesteps)):\n","    prob = model.predict(X[ind,:], batch_size=bs)\n","    cumul[:,1:] = np.cumsum(prob, axis = 1)\n","    for i in range(bs):\n","            enc[i].write(cumul[i,:], y_original[ind[i]])\n","    ind = ind + 1\n","else:"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uhxTqspYmneI","outputId":"3d09c08d-fd9a-4bb9-f5fc-ef26e9df120f","colab":{}},"source":["DZ_model = load_model('my_model.h5', compile=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\quoca\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F0QbUm_SmneS","outputId":"98270476-d5b8-4de8-a70a-784bb8e14d9f","colab":{}},"source":["#Create Uniform distribution to feed in to the model\n","alphabet_size = 4\n","prob = np.ones(alphabet_size)/alphabet_size\n","#make an array of cumulative probabilities\n","c = []\n","for i in range(len(prob)+1):\n","    c.append(sum(prob[0:i]))\n","\n","cumul = np.zeros((bs, alphabet_size+1), dtype = np.uint64)  \n","for j in (range(num_iters - timesteps)):\n","    prob = model.predict(X[ind,:], batch_size=bs)\n","    cumul[:,1:] = np.cumsum(prob, axis = 1)\n","    for i in range(bs):\n","        new = enc.write(Y_raw[ind[i]], cumul[i,:].tolist())\n","    ind = ind + 1\n","    low_final = new[0] #fina low value \n","    s_final = new[1]  #count of renormalizations\n","    bitstream = format(int(low_final), 'b')  \n","while len(bitstream) < (s_final+bitprecision):\n","    bitstream = '0' + bitstream\n","    print(\"encoded bit length:\", len(bitstream))\n","\n","\n","    #START DECODER\n","    dec = ArithmeticDecoder(bitprecision, bitstream, c)\n","\n","    symdec = []\n","    for symbols in range(len(sym)):\n","        symdec.append(dec.decode())\n","    print(\"decoded num stream:\", symdec)\n","\n","    #convert numbers back to letters\n","    dec_stream = []\n","    for x, sym in enumerate(symdec): \n","        if sym == 0:\n","            dec_stream.append('a')\n","        elif sym ==1:\n","            dec_stream.append('c')\n","        elif sym == 2:\n","            dec_stream.append('g')\n","        elif sym == 3:\n","            dec_stream.append('t')\n","\n","    dec_stream = dec_stream[:-1] \n","    print(\"decoded symbol stream:\", dec_stream)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'timesteps' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-10-85511f7a46cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mcumul\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malphabet_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcumul\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'timesteps' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GG5yBoVvmneX","outputId":"248e8cb9-9298-4373-8329-f8a2d6abf98f","colab":{}},"source":["cumul"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0]], dtype=uint64)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KDyGTbPumneb","outputId":"af567d31-bdbf-4cc5-fd16-f7d33ea9d473","colab":{}},"source":["\n","print(X)\n","print(X[ind,:].shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0 3 3 ... 3 3 1]\n"," [3 3 1 ... 3 1 2]\n"," [3 1 2 ... 1 2 2]\n"," ...\n"," [3 0 0 ... 1 1 1]\n"," [0 0 3 ... 1 1 3]\n"," [0 3 0 ... 1 3 1]]\n","(128, 64)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HyEMSoN_mnef","outputId":"06f5bae6-206d-4178-947d-a15f531798a2","colab":{}},"source":["cumul = np.zeros((bs, alphabet_size+1), dtype = np.uint64)  \n","print(cumul[0,:].tolist())\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mI44jpHCmnej","outputId":"d3254263-3029-4f0d-b281-7966f0b6d8b0","colab":{}},"source":["print(sum(prob[1,:]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.0000000596046448\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SCEJCyyVmnem","outputId":"e6a166ca-e78c-417d-da5b-e931e1d88991","colab":{}},"source":["import arithmetic_coder"],"execution_count":0,"outputs":[{"output_type":"stream","text":["symbol stream input: ['a' 'c' 'g' 't' 't' 'a' 'c' 't' 'g' 'a' 't' 'a' 'a' 't' 'a' 'c' 'c' 'g'\n"," 't']\n","num stream: [0, 1, 2, 3, 3, 0, 1, 3, 2, 0, 3, 0, 0, 3, 0, 1, 1, 2, 3]\n","probability: [0.5 0.2 0.2 0.1]\n","cumulative prob: [0, 0.5, 0.7, 0.8999999999999999, 0.9999999999999999]\n","encoded bit length: 106\n","decoded num stream: [0, 1, 2, 3, 3, 0, 1, 3, 2, 0, 3, 0, 0, 3, 0, 1, 1, 2, 3, 1]\n","decoded symbol stream: ['a', 'c', 'g', 't', 't', 'a', 'c', 't', 'g', 'a', 't', 'a', 'a', 't', 'a', 'c', 'c', 'g', 't']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tLG3fhETmnep","colab":{}},"source":["prob = np.ones(alphabet_size)/alphabet_size\n","cumul = np.zeros(alphabet_size+1, dtype = float)\n","cumul[1:] = np.cumsum(prob)        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BTwOxFzSmnet","outputId":"eec89394-c910-4c29-f930-a48fd8eaeeb8","colab":{}},"source":["cumul"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.  , 0.25, 0.5 , 0.75, 1.  ])"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fKHyogiWmnex","colab":{}},"source":["prob = np.ones(alphabet_size)/alphabet_size\n","cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n","cumul[1:] = np.cumsum(prob*10000000 + 1)   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"34WIkqaqmnez","outputId":"2b378c9d-a247-4785-c00e-77e52ffdebfa","colab":{}},"source":["cumul"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([       0,  2500001,  5000002,  7500003, 10000004], dtype=uint64)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JmVjbzQEmne2","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}